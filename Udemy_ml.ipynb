{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkQXqy3lLRRC/y1snPJ7i4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamilmech/Udemy_ML_Course/blob/main/Udemy_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1IqQnSr2cwh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression\n",
        "\n",
        "y is the dependent variable (the value you're predicting or measuring).\n",
        "\n",
        "x is the independent variable (the input or feature).\n",
        "\n",
        "m is the slope of the line, indicating how much\n",
        "\n",
        "c is the y-intercept, which is the value of\n",
        "\n",
        "\n",
        "$$y = mx + c$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "484ilXLj2reb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EKgy2hsKZifi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v630HqHcZidt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gSFCsneRZib6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MSE (Mean Squared Error):\n",
        " The average of the squared differences between predicted and actual values, giving more weight to larger errors.\n",
        "\n",
        "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dcG9lPAPRh-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MSE Example:\n",
        " If predicted prices are $200k, $250k and actual prices are $210k, $240k, the MSE is $100k²."
      ],
      "metadata": {
        "id": "SsvSNSklRnmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression:\n",
        "\n",
        " A method to model the relationship between a dependent variable and one or more independent variables using a straight line."
      ],
      "metadata": {
        "id": "ZF8-g_UE5hTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " ## MAE (Mean Absolute Error):\n",
        "\n",
        " The average of absolute differences between predicted and actual values, measuring model accuracy in the same units as the data.\n",
        "\n",
        "\n",
        "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|^2$$"
      ],
      "metadata": {
        "id": "-sI8vIQI5jzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### MAE Example:\n",
        " If predicted prices are $200k, $250k and actual prices are $210k, $240k, the MAE is $10k."
      ],
      "metadata": {
        "id": "GSJVGbnPRPvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSE (Root Mean Squared Error):\n",
        "The square root of the average squared differences between predicted and actual values, combining the advantages of MSE and interpretability in original units.\n",
        "\n",
        "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n"
      ],
      "metadata": {
        "id": "B_LXuxJzP-Es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RMSE Example:\n",
        " If the MSE is $100k², the RMSE is $10k."
      ],
      "metadata": {
        "id": "eqUg0XzMRxfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Difference Between Point and Plane:\n",
        "The vertical distance between a data point and the predicted value on a regression line or plane.\n",
        "\n"
      ],
      "metadata": {
        "id": "FHBtrtqeQGbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RnVPLlRC2ojd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instance-Based Learning:\n",
        " A learning approach where the model memorizes instances of the training data and makes predictions by comparing new instances to stored ones (e.g., k-NN)."
      ],
      "metadata": {
        "id": "4PVUKD1CQLWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model-Based Learning:\n",
        "A learning approach where the model learns a function or rule from the training data to make predictions on new data (e.g., linear regression)."
      ],
      "metadata": {
        "id": "nKBQuVHoQRdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Slope:\n",
        " The rate at which the dependent variable changes with respect to the independent variable, indicating the steepness of the line."
      ],
      "metadata": {
        "id": "gPrZONT_QYTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cost Function:\n",
        " A function that measures the error of the model's predictions, used to guide the optimization process (e.g., MSE in linear regression).\n",
        "\n",
        "\n",
        " $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$"
      ],
      "metadata": {
        "id": "CGtIxRzdQflx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cost Function Example:\n",
        " Using MSE to measure the error between predicted and actual house prices and adjusting the model to minimize this error.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4nBt_NtNR5y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convergence Algorithm:\n",
        " An algorithm that iteratively adjusts model parameters to minimize the cost function, like gradient descent."
      ],
      "metadata": {
        "id": "i6kOFZbDQlvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convergence Algorithm Example:\n",
        " Using gradient descent to adjust the line of best fit until the error cannot be reduced further."
      ],
      "metadata": {
        "id": "LKAyl901R_TC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performance Metrics:\n",
        "Measures used to evaluate the accuracy and effectiveness of a model (e.g., MAE, MSE, RMSE)."
      ],
      "metadata": {
        "id": "0E6ClhQuQ19A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performance Metrics Example:\n",
        "Evaluating a model predicting house prices by calculating its RMSE."
      ],
      "metadata": {
        "id": "2Szk7DueSFS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization\n",
        "\n",
        "its one type of linear regerssion used to prevent overfitting and imporve the  generalization performance of model\n",
        "\n",
        "\n",
        "`Regularization  = loss funtion + Penalty`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6wfWOtfuVX4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge Regularization\n",
        "\n",
        "l2  Regularization adds a penalty term with the loss funtion , which keeps the magnitude of the models weights(coefficients) as small as possible\n",
        "\n"
      ],
      "metadata": {
        "id": "hxAuSAuGdljh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2$$\n",
        "\n"
      ],
      "metadata": {
        "id": "4pWyyCgRXPHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lasso  Regularization\n",
        "\n",
        "Lasso (L1) Regularization is a technique used in machine learning to prevent overfitting by adding a penalty equal to the absolute value of the coefficients to the cost function. It tends to reduce some coefficients to exactly zero, effectively selecting a simpler model by keeping only the most important features."
      ],
      "metadata": {
        "id": "BRCPUO3PeZwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|$$\n"
      ],
      "metadata": {
        "id": "Y3f4VIjLfwZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression (L2 Regularization)\n",
        "\n",
        "The cost function for Ridge regression adds a penalty equal to the sum of the squared values of the coefficients:\n",
        "\n",
        "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2$$\n",
        "\n",
        "---\n",
        "\n",
        "### Lasso Regression (L1 Regularization)\n",
        "\n",
        "The cost function for Lasso regression adds a penalty equal to the sum of the absolute values of the coefficients:\n",
        "\n",
        "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|$$\n",
        "\n",
        "---\n",
        "\n",
        "### Elastic Net Regularization\n",
        "\n",
        "The cost function for Elastic Net regularization combines both L1 and L2 penalties:\n",
        "\n",
        "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2 + \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\lambda_2 \\sum_{j=1}^{n} \\theta_j^2$$\n"
      ],
      "metadata": {
        "id": "B0rZtTEygF8K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rH1oZ_KiW_v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias\n",
        "The bias is the difference between our actual and predicted values"
      ],
      "metadata": {
        "id": "TdY4zx2eTU8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## variance\n",
        "Variance is the sensitivity of the model to data fluctuation and noises"
      ],
      "metadata": {
        "id": "mfJCPG-uUXhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Overfitting:\n",
        " When a model learns too much from the training data, capturing noise and details that don't generalize well to new data. Example: A complex model that performs well on training data but poorly on test data.\n",
        "\n",
        "##Underfitting:\n",
        " When a model is too simple and fails to capture the underlying pattern of the data, leading to poor performance on both training and test data. Example: A linear model used for data with a non-linear relationship."
      ],
      "metadata": {
        "id": "nCpvQKUATojR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSc9PAsYbZbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-validation** is a technique used to evaluate the performance of a machine learning model by dividing the data into subsets, training the model on some of these subsets, and testing it on the remaining ones. This helps in assessing how well the model generalizes to unseen data.\n",
        "\n",
        "### Types of Cross-Validation:\n",
        "\n",
        "1. **Holdout Validation**:\n",
        "   - **Definition**: The dataset is split into two parts: one for training and one for testing.\n",
        "   - **Example**: Split a dataset of 1000 samples into 800 for training and 200 for testing.\n",
        "\n",
        "2. **K-Fold Cross-Validation**:\n",
        "   - **Definition**: The dataset is divided into \\(k\\) equal parts (folds). The model is trained on \\(k-1\\) folds and tested on the remaining fold. This process is repeated \\(k\\) times, with each fold being used as the test set once.\n",
        "   - **Example**: For a 5-Fold CV, a dataset of 1000 samples is split into 5 parts of 200 each. The model is trained on 4 parts and tested on 1 part, repeated 5 times.\n",
        "\n",
        "3. **Leave-One-Out Cross-Validation (LOOCV)**:\n",
        "   - **Definition**: Each data point in the dataset is used as a test set while the remaining points are used as the training set. This is repeated for each data point.\n",
        "   - **Example**: For a dataset with 1000 samples, the model is trained on 999 samples and tested on 1, repeated 1000 times.\n",
        "\n",
        "4. **Stratified K-Fold Cross-Validation**:\n",
        "   - **Definition**: Similar to K-Fold but ensures that each fold has the same proportion of classes as the original dataset, which is useful for imbalanced datasets.\n",
        "   - **Example**: For a 5-Fold CV on a dataset with 70% class A and 30% class B, each fold will have the same 70/30 class distribution.\n",
        "\n",
        "5. **Time Series Cross-Validation**:\n",
        "   - **Definition**: Used for time series data where the order of data points matters. The training set is built using a rolling or expanding window approach to predict the next data points.\n",
        "   - **Example**: In a time series of 1000 data points, train on points 1-800 to predict 801-820, then use 1-820 to predict 821-840, and so on.\n",
        "\n",
        "### Sample Example:\n",
        "\n",
        "**5-Fold Cross-Validation Example**:\n",
        "- **Step 1**: Split a dataset of 100 samples into 5 folds (20 samples each).\n",
        "- **Step 2**: Train the model on 4 folds (80 samples) and test on the 1 remaining fold (20 samples).\n",
        "- **Step 3**: Repeat this process 5 times, using each fold as the test set once.\n",
        "- **Step 4**: Average the performance metrics across the 5 iterations to evaluate the model.\n",
        "\n",
        "Cross-validation provides a more reliable estimate of model performance by reducing the bias that might result from a single train-test split."
      ],
      "metadata": {
        "id": "N6fUxFLzm88u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Linear Regression\n",
        "\n",
        "## Definition\n",
        "Multiple linear regression is a statistical technique that models the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "## Formula\n",
        "\n",
        "The general formula for multiple linear regression is:\n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
        "$$\n",
        "\n",
        "\n",
        "## Key Points\n",
        "- **Purpose**: To predict the dependent variable using multiple factors.\n",
        "- **Coefficients**: Indicate the influence of each independent variable on the dependent variable.\n",
        "- **Assumption**: Linear relationship between dependent and independent variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "6qtIJnAwo4hW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "## Definition\n",
        "Logistic regression is a statistical method used for binary classification problems. It models the probability that a given input belongs to a certain class.\n",
        "\n",
        "## Mathematical Intuition\n",
        "\n",
        "1. **Sigmoid Function**:\n",
        "   Logistic regression uses the sigmoid function to map predicted values to probabilities between 0 and 1.\n",
        "\n",
        "   $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "   Where:\n",
        "   - **\\( \\sigma(z) \\)**: The sigmoid function output (probability).\n",
        "   - **\\( e \\)**: Base of the natural logarithm.\n",
        "   - **\\( z \\)**: Linear combination of the input features and coefficients.\n",
        "\n",
        "2. **Linear Model**:\n",
        "   The input features are combined linearly:\n",
        "\n",
        "   $$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n$$\n",
        "\n",
        "\n",
        "3. **Probability Prediction**:\n",
        "   The probability of the positive class is given by:\n",
        "\n",
        "   $$P(y = 1 \\mid x) = \\sigma(z) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}}$$\n",
        "\n",
        "   The probability of the negative class is:\n",
        "\n",
        "   $$P(y = 0 \\mid x) = 1 - \\sigma(z)$$\n",
        "\n",
        "4. **Decision Boundary**:\n",
        "   The model classifies input as class 1 if the predicted probability is greater than 0.5 and as class 0 otherwise.\n",
        "\n",
        "   $$\\text{Class} = \\begin{cases}\n",
        "   1 & \\text{if } \\sigma(z) > 0.5 \\\\\n",
        "   0 & \\text{otherwise}\n",
        "   \\end{cases}$$\n",
        "\n",
        "## Key Points\n",
        "- **Purpose**: To estimate the probability of a binary outcome.\n",
        "- **Sigmoid Function**: Maps predictions to a probability between 0 and 1.\n",
        "- **Decision Rule**: Uses a threshold (typically 0.5) to classify outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "rDWnYKoHp_iZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression with One-vs-Rest (OVR)\n",
        "\n",
        "## Definition\n",
        "Logistic Regression with One-vs-Rest (OVR) is used for multi-class classification problems. It involves training multiple binary classifiers, each for a single class versus all other classes.\n",
        "\n",
        "## Method\n",
        "\n",
        "1. **Binary Classifier for Each Class**:\n",
        "   - Train one logistic regression model for each class, treating it as the positive class and all other classes as the negative class.\n",
        "   - If there are \\(K\\) classes, \\(K\\) separate binary classifiers are trained.\n",
        "\n",
        "2. **Prediction**:\n",
        "   - Each classifier outputs a probability for its class.\n",
        "   - The class with the highest probability among all classifiers is chosen as the predicted class.\n",
        "\n",
        "## Mathematical Formulation\n",
        "\n",
        "1. **Binary Logistic Model**:\n",
        "   For class \\(k\\), the probability is:\n",
        "\n",
        "   $$P(y = k \\mid x) = \\frac{1}{1 + e^{-(\\beta_{0k} + \\beta_{1k} x_1 + \\beta_{2k} x_2 + \\dots + \\beta_{nk} x_n)}}$$\n",
        "\n",
        "2. **Prediction Rule**:\n",
        "   - Compute the probability for each class \\(k\\).\n",
        "   - Assign the input to the class with the highest probability:\n",
        "\n",
        "   $$\\text{Class} = \\arg\\max_k P(y = k \\mid x)$$\n",
        "\n",
        "## Key Points\n",
        "- **Purpose**: To extend logistic regression to multi-class problems.\n",
        "- **OVR Approach**: Treats each class separately and compares probabilities.\n",
        "- **Output**: The class with the highest probability is chosen.\n",
        "\n"
      ],
      "metadata": {
        "id": "UGbWkxwXr6PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search\n",
        "\n",
        "## Definition\n",
        "Grid Search is a method used to find the best hyperparameters for a machine learning model by testing all possible combinations from a predefined set of hyperparameter values.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Define a Grid**:\n",
        "   - Create a list of possible values for each hyperparameter.\n",
        "   - Example: For `C` in Support Vector Machines, you might test values `[0.1, 1, 10]`.\n",
        "\n",
        "2. **Create Combinations**:\n",
        "   - Combine all possible values of the hyperparameters into different sets.\n",
        "   - Example: If testing `C` and `gamma`, you test combinations like `(0.1, 0.01)`, `(0.1, 0.1)`, `(1, 0.01)`, and so on.\n",
        "\n",
        "3. **Evaluate**:\n",
        "   - Train and test the model for each combination of hyperparameters.\n",
        "   - Measure the performance using metrics like accuracy or F1 score.\n",
        "\n",
        "4. **Select Best**:\n",
        "   - Choose the combination that gives the best performance.\n",
        "\n",
        "## Example\n",
        "\n",
        "Imagine you want to find the best hyperparameters for a model with two parameters: `learning_rate` and `n_estimators`.\n",
        "\n",
        "- **Define Grid**:\n",
        "  - `learning_rate`: `[0.01, 0.1, 1]`\n",
        "  - `n_estimators`: `[50, 100, 200]`\n",
        "\n",
        "- **Combinations**:\n",
        "  - `(0.01, 50)`, `(0.01, 100)`, `(0.01, 200)`\n",
        "  - `(0.1, 50)`, `(0.1, 100)`, `(0.1, 200)`\n",
        "  - `(1, 50)`, `(1, 100)`, `(1, 200)`\n",
        "\n",
        "- **Evaluate**:\n",
        "  - Train and test the model for each combination.\n",
        "\n",
        "- **Select Best**:\n",
        "  - Choose the combination with the highest accuracy or best performance metric.\n",
        "\n",
        "## Key Points\n",
        "- **Purpose**: To find the best hyperparameters for a model.\n",
        "- **Process**: Test all combinations from a predefined grid.\n",
        "- **Outcome**: The combination that performs best on validation data is selected.\n",
        "\n"
      ],
      "metadata": {
        "id": "pIxgRSDVtRyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Pickling\n",
        "\n",
        "## Definition\n",
        "Model pickling is the process of saving a trained machine learning model to a file so it can be loaded and used later without retraining.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Train Your Model**:\n",
        "   - First, you train a machine learning model on your data.\n",
        "\n",
        "2. **Save the Model**:\n",
        "   - Use a method called \"pickling\" to save the trained model to a file.\n",
        "   - This process converts the model into a format that can be stored and transferred.\n",
        "\n",
        "3. **Load the Model**:\n",
        "   - When needed, you can load the saved model from the file.\n",
        "   - This allows you to use the model for making predictions without retraining.\n",
        "\n",
        "## Example\n",
        "\n",
        "1. **Training and Saving**:\n",
        "   ```python\n",
        "   import pickle\n",
        "   from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "   # Train a model\n",
        "   model = LogisticRegression()\n",
        "   model.fit(X_train, y_train)\n",
        "\n",
        "   # Save the model\n",
        "   with open('model.pkl', 'wb') as file:\n",
        "       pickle.dump(model, file)\n"
      ],
      "metadata": {
        "id": "SD5lhfc4u1zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomized Search CV\n",
        "\n",
        "## Definition\n",
        "Randomized Search CV is a method used to find the best hyperparameters for a machine learning model by randomly sampling from a range of possible values instead of trying all combinations.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Define a Parameter Grid**:\n",
        "   - Create a range of values for each hyperparameter, but you don’t need to test all possible combinations.\n",
        "\n",
        "2. **Random Sampling**:\n",
        "   - Randomly select a subset of combinations from the parameter grid.\n",
        "   - For each combination, train and evaluate the model.\n",
        "\n",
        "3. **Evaluate Performance**:\n",
        "   - Choose the combination that performs the best based on your evaluation metric (e.g., accuracy).\n",
        "\n",
        "## Example\n",
        "\n",
        "1. **Define Parameter Grid**:\n",
        "   ```python\n",
        "   param_distributions = {\n",
        "       'n_estimators': [50, 100, 200],\n",
        "       'max_depth': [None, 10, 20],\n",
        "       'learning_rate': [0.01, 0.1, 1]\n",
        "   }\n"
      ],
      "metadata": {
        "id": "wadtlngBv4af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression with One-vs-Rest (OVR)\n",
        "\n",
        "## Definition\n",
        "Logistic Regression with One-vs-Rest (OVR) is a technique for handling multi-class classification problems. It works by training multiple binary classifiers, each one distinguishing a single class from all others.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Train Classifiers**:\n",
        "   - For each class \\( k \\), train a logistic regression model to separate that class from all other classes.\n",
        "\n",
        "2. **Predict Class**:\n",
        "   - Each classifier outputs a probability that the input belongs to its class.\n",
        "   - The class with the highest probability is chosen as the final prediction.\n",
        "\n",
        "## Example\n",
        "\n",
        "**Scenario**: Classify fruits into Apples, Bananas, or Cherries based on weight and color.\n",
        "\n",
        "1. **Train Classifiers**:\n",
        "   - **Apple Classifier**: Distinguishes Apples from Non-Apples (Bananas and Cherries).\n",
        "   - **Banana Classifier**: Distinguishes Bananas from Non-Bananas (Apples and Cherries).\n",
        "   - **Cherry Classifier**: Distinguishes Cherries from Non-Cherries (Apples and Bananas).\n",
        "\n",
        "2. **Formulas**:\n",
        "   - **Apple Classifier**:\n",
        "     ```\n",
        "     P(Apple | Weight, Color) = 1 / (1 + exp(-(β0_Apple + β1_Apple * Weight + β2_Apple * Color)))\n",
        "     ```\n",
        "   - **Banana Classifier**:\n",
        "     ```\n",
        "     P(Banana | Weight, Color) = 1 / (1 + exp(-(β0_Banana + β1_Banana * Weight + β2_Banana * Color)))\n",
        "     ```\n",
        "   - **Cherry Classifier**:\n",
        "     ```\n",
        "     P(Cherry | Weight, Color) = 1 / (1 + exp(-(β0_Cherry + β1_Cherry * Weight + β2_Cherry * Color)))\n",
        "     ```\n",
        "\n",
        "3. **Prediction**:\n",
        "   - Suppose you have a fruit with weight = 150 grams and color = 0.8.\n",
        "   - Calculate probabilities:\n",
        "     - **Apple**: P(Apple) = 0.7\n",
        "     - **Banana**: P(Banana) = 0.2\n",
        "     - **Cherry**: P(Cherry) = 0.1\n",
        "   - **Predicted Class**: **Apple** (highest probability).\n",
        "\n",
        "## Key Points\n",
        "- **Purpose**: To extend logistic regression to handle multiple classes.\n",
        "- **OVR Approach**: One classifier per class, each distinguishing its class from the rest.\n",
        "- **Output**: The class with the highest predicted probability is selected.\n"
      ],
      "metadata": {
        "id": "IfN5J_8Gwmsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression ROC Curve\n",
        "\n",
        "## Definition\n",
        "The ROC curve is a graphical representation used to evaluate the performance of a binary classification model, such as logistic regression. It shows the trade-off between the True Positive Rate (TPR) and the False Positive Rate (FPR) at various threshold settings.\n",
        "\n",
        "## Key Terms\n",
        "\n",
        "1. **True Positive Rate (TPR)**:\n",
        "   - Also known as Sensitivity or Recall.\n",
        "   - Measures the proportion of actual positives correctly identified.\n",
        "   - Formula: $$\n",
        "   \\text{TPR} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
        "   $$\n",
        "\n",
        "2. **False Positive Rate (FPR)**:\n",
        "   - Measures the proportion of actual negatives incorrectly identified as positives.\n",
        "   - Formula: $$\n",
        "   \\text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}\n",
        "   $$\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Calculate TPR and FPR**:\n",
        "   - For different threshold values, calculate the TPR and FPR.\n",
        "\n",
        "2. **Plot ROC Curve**:\n",
        "   - Plot TPR on the y-axis and FPR on the x-axis.\n",
        "\n",
        "3. **Interpret ROC Curve**:\n",
        "   - A curve closer to the top-left corner indicates a better model.\n",
        "   - The area under the ROC curve (AUC) represents the model's ability to discriminate between classes.\n",
        "\n",
        "## Example\n",
        "\n",
        "1. **Train a Logistic Regression Model**:\n",
        "   ```python\n",
        "   from sklearn.linear_model import LogisticRegression\n",
        "   from sklearn.metrics import roc_curve, auc\n",
        "   import matplotlib.pyplot as plt\n",
        "\n",
        "   # Train model\n",
        "   model = LogisticRegression()\n",
        "   model.fit(X_train, y_train)\n",
        "\n",
        "   # Predict probabilities\n",
        "   y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "   # Calculate ROC curve\n",
        "   fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "   roc_auc = auc(fpr, tpr)\n",
        "\n",
        "   # Plot ROC curve\n",
        "   plt.figure()\n",
        "   plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "   plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "   plt.xlim([0.0, 1.0])\n",
        "   plt.ylim([0.0, 1.05])\n",
        "   plt.xlabel('False Positive Rate')\n",
        "   plt.ylabel('True Positive Rate')\n",
        "   plt.title('Receiver Operating Characteristic')\n",
        "   plt.legend(loc='lower right')\n",
        "   plt.show()\n"
      ],
      "metadata": {
        "id": "BSIiMu0hxWEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM Diagram\n",
        "\n",
        "## Simple Linear SVM Diagram\n",
        "\n",
        "![Support Vector Machine Diagram](https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png)\n"
      ],
      "metadata": {
        "id": "GNgbIgrp3B4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machines (SVM)\n",
        "\n",
        "## Definition\n",
        "Support Vector Machines (SVM) are supervised learning algorithms used for classification and regression tasks. They find the optimal boundary (hyperplane) that separates different classes in the feature space.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "1. **Hyperplane**:\n",
        "   - A decision boundary that separates different classes.\n",
        "   - In 2D, it’s a line; in 3D, it’s a plane; in higher dimensions, it’s a hyperplane.\n",
        "\n",
        "2. **Support Vectors**:\n",
        "   - Data points that are closest to the hyperplane.\n",
        "   - These points are critical as they define the margin of the classifier.\n",
        "\n",
        "3. **Margin**:\n",
        "   - The distance between the hyperplane and the closest data points (support vectors).\n",
        "   - SVM aims to maximize this margin.\n",
        "\n",
        "4. **Kernel Trick**:\n",
        "   - Used to transform non-linearly separable data into linearly separable data by mapping it to a higher-dimensional space.\n",
        "   - Common kernels include linear, polynomial, and radial basis function (RBF).\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Find the Optimal Hyperplane**:\n",
        "   - SVM selects the hyperplane that maximizes the margin between classes.\n",
        "\n",
        "2. **Transform Data if Needed**:\n",
        "   - If data is not linearly separable, apply a kernel function to map it to a higher-dimensional space.\n",
        "\n",
        "3. **Classify New Data**:\n",
        "   - Use the trained hyperplane to classify new data points.\n",
        "\n",
        "## Example\n",
        "\n",
        "1. **Simple 2D Classification**:\n",
        "   ```python\n",
        "   from sklearn import datasets\n",
        "   from sklearn.svm import SVC\n",
        "   import matplotlib.pyplot as plt\n",
        "   import numpy as np\n",
        "\n",
        "   # Load dataset\n",
        "   iris = datasets.load_iris()\n",
        "   X = iris.data[:, :2]  # Use only the first two features\n",
        "   y = iris.target\n",
        "\n",
        "   # Train SVM model\n",
        "   model = SVC(kernel='linear', C=1.0)\n",
        "   model.fit(X, y)\n",
        "\n",
        "   # Plot decision boundary\n",
        "   h = .02\n",
        "   x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "   y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "   xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                        np.arange(y_min, y_max, h))\n",
        "   Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "   Z = Z.reshape(xx.shape)\n",
        "\n",
        "   plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "   plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', marker='o')\n",
        "   plt.title('SVM Classification')\n",
        "   plt.xlabel('Feature 1')\n",
        "   plt.ylabel('Feature 2')\n",
        "   plt.show()\n"
      ],
      "metadata": {
        "id": "GrlS1YtgymXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "97dN9ujKzxVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hard Margin vs. Soft Margin SVM\n",
        "\n",
        "## Hard Margin SVM\n",
        "\n",
        "### Definition\n",
        "Hard Margin SVM is a type of SVM that requires the data to be linearly separable with no misclassifications. It tries to find a hyperplane that perfectly separates the classes.\n",
        "\n",
        "### Characteristics\n",
        "- **Strict Separation**: Assumes that data is perfectly linearly separable.\n",
        "- **No Tolerance for Misclassification**: Cannot handle noisy data or outliers well.\n",
        "- **Optimal Hyperplane**: Maximizes the margin while ensuring no misclassification.\n",
        "\n",
        "### Example\n",
        "- **Use Case**: Ideal for datasets where classes are perfectly separable with a clear margin.\n",
        "\n",
        "## Soft Margin SVM\n",
        "\n",
        "### Definition\n",
        "Soft Margin SVM allows for some misclassifications in order to achieve better generalization on real-world, non-linearly separable data. It introduces a penalty for misclassifications.\n",
        "\n",
        "### Characteristics\n",
        "- **Flexible Separation**: Allows some data points to be within the margin or on the wrong side of the hyperplane.\n",
        "- **Tolerance for Misclassification**: Can handle noisy data and outliers better.\n",
        "- **Regularization Parameter (C)**: Controls the trade-off between maximizing the margin and minimizing classification error.\n",
        "\n",
        "### Example\n",
        "- **Use Case**: Useful for datasets with overlapping classes or noisy data.\n",
        "\n",
        "## Comparison\n",
        "\n",
        "| Aspect              | Hard Margin SVM                 | Soft Margin SVM                  |\n",
        "|---------------------|----------------------------------|----------------------------------|\n",
        "| **Data Separation** | Perfectly linearly separable     | Allows some misclassifications   |\n",
        "| **Tolerance**       | No tolerance for misclassification | Tolerates some misclassifications |\n",
        "| **Regularization**  | No regularization parameter      | Regularization parameter (C)      |\n",
        "\n",
        "## Key Points\n",
        "- **Hard Margin**: Requires perfect separation; no tolerance for errors.\n",
        "- **Soft Margin**: Allows some errors; more flexible and robust to noisy data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fz0cN-Yu0rE0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qX6mujPamZwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical Intuition of Support Vector Machines (SVMs)\n",
        "\n",
        "## Objective\n",
        "SVM aims to find the optimal hyperplane that maximizes the margin between two classes.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "1. **Hyperplane**:\n",
        "   - A decision boundary that separates different classes.\n",
        "   - In 2D, it’s a line; in 3D, it’s a plane; in higher dimensions, it’s a hyperplane.\n",
        "   - Equation of hyperplane: $$\n",
        "   w^T x + b = 0\n",
        "   $$\n",
        "   where \\( w \\) is the weight vector and \\( b \\) is the bias.\n",
        "\n",
        "2. **Margin**:\n",
        "   - The distance between the hyperplane and the nearest data points (support vectors).\n",
        "   - The goal is to maximize this margin.\n",
        "   - For a hyperplane, the margin is: $$\n",
        "   \\text{Margin} = \\frac{2}{\\|w\\|}\n",
        "   $$\n",
        "\n",
        "3. **Support Vectors**:\n",
        "   - Data points closest to the hyperplane.\n",
        "   - These points are used to define the margin and are crucial for finding the optimal hyperplane.\n",
        "\n",
        "4. **Optimization Problem**:\n",
        "   - SVM solves a quadratic optimization problem to find the optimal hyperplane.\n",
        "   - **Objective**: Maximize the margin, which is equivalent to minimizing: $$\n",
        "   \\frac{1}{2} \\|w\\|^2\n",
        "   $$\n",
        "   subject to constraints: $$\n",
        "   y_i (w^T x_i + b) \\geq 1 \\text{ for all } i\n",
        "   $$\n",
        "   where \\( y_i \\) is the class label of \\( x_i \\).\n",
        "\n",
        "5. **Soft Margin (for non-linearly separable data)**:\n",
        "   - Allows some data points to be misclassified.\n",
        "   - Introduces a penalty parameter \\( C \\) to control the trade-off between margin size and classification error.\n",
        "   - **Objective**: Minimize: $$\n",
        "   \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^m \\xi_i\n",
        "   $$\n",
        "   subject to: $$\n",
        "   y_i (w^T x_i + b) \\geq 1 - \\xi_i\n",
        "   $$\n",
        "   where \\( \\xi_i \\) are slack variables representing the amount of misclassification.\n",
        "\n",
        "6. **Kernel Trick**:\n",
        "   - Used to handle non-linearly separable data.\n",
        "   - Maps data to a higher-dimensional space where a linear separation is possible.\n",
        "   - **Kernel Function**: \\( K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j) \\)\n",
        "   where \\( \\phi \\) is a feature mapping function.\n",
        "\n",
        "## Summary\n",
        "- **Goal**: Find the optimal hyperplane to maximize the margin.\n",
        "- **Hard Margin**: Perfect separation with no misclassification.\n",
        "- **Soft Margin**: Allows some misclassifications with a regularization parameter \\( C \\).\n",
        "- **Kernel Trick**: Transforms data to enable separation in higher dimensions.\n",
        "\n"
      ],
      "metadata": {
        "id": "GWKUgbHX10-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cost Function for Support Vector Machines (SVMs)\n",
        "\n",
        "## Objective\n",
        "The cost function for SVM aims to find the optimal hyperplane that maximizes the margin between classes while minimizing classification errors.\n",
        "\n",
        "## Hard Margin SVM\n",
        "\n",
        "### Cost Function\n",
        "For linearly separable data, the cost function is: $$\n",
        "J(w, b) = \\frac{1}{2} \\|w\\|^2\n",
        "$$\n",
        "- **Objective**: Minimize \\( \\frac{1}{2} \\|w\\|^2 \\) to maximize the margin between classes.\n",
        "- **Constraints**: Ensure that all data points are correctly classified:\n",
        "  $$\n",
        "  y_i (w^T x_i + b) \\geq 1 \\text{ for all } i\n",
        "  $$\n",
        "  where \\( y_i \\) is the class label, \\( x_i \\) is the feature vector, \\( w \\) is the weight vector, and \\( b \\) is the bias.\n",
        "\n",
        "## Soft Margin SVM\n",
        "\n",
        "### Cost Function\n",
        "For non-linearly separable data, the cost function incorporates slack variables \\( \\xi_i \\) to allow some misclassifications:\n",
        "$$\n",
        "J(w, b, \\xi) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^m \\xi_i\n",
        "$$\n",
        "- **Objective**: Minimize \\( \\frac{1}{2} \\|w\\|^2 \\) to maximize the margin while penalizing misclassifications with \\( C \\sum_{i=1}^m \\xi_i \\).\n",
        "- **Constraints**: Ensure that slack variables \\( \\xi_i \\) satisfy:\n",
        "  $$\n",
        "  y_i (w^T x_i + b) \\geq 1 - \\xi_i\n",
        "  $$\n",
        "  and \\( \\xi_i \\geq 0 \\).\n",
        "\n",
        "## Key Points\n",
        "- **Hard Margin SVM**: Cost function focuses solely on maximizing the margin with perfect separation.\n",
        "- **Soft Margin SVM**: Includes a penalty term to handle misclassifications and allows for some flexibility.\n",
        "- **Regularization Parameter (C)**: Controls the trade-off between maximizing the margin and minimizing classification error.\n",
        "\n"
      ],
      "metadata": {
        "id": "bWqcF9fC2J63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qub82Ovz2jts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "8YVqynWb27wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nbhmpVVVsswV"
      }
    }
  ]
}