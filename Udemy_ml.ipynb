{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMf5dHW6qoUhAPwto/mDo1e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamilmech/Udemy_ML_Course/blob/main/Udemy_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1IqQnSr2cwh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression\n",
        "\n",
        "y is the dependent variable (the value you're predicting or measuring).\n",
        "\n",
        "x is the independent variable (the input or feature).\n",
        "\n",
        "m is the slope of the line, indicating how much\n",
        "\n",
        "c is the y-intercept, which is the value of\n",
        "\n",
        "\n",
        "$$y = mx + c$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "484ilXLj2reb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EKgy2hsKZifi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v630HqHcZidt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gSFCsneRZib6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MSE (Mean Squared Error):\n",
        " The average of the squared differences between predicted and actual values, giving more weight to larger errors.\n",
        "\n",
        "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dcG9lPAPRh-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MSE Example:\n",
        " If predicted prices are $200k, $250k and actual prices are $210k, $240k, the MSE is $100k²."
      ],
      "metadata": {
        "id": "SsvSNSklRnmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression:\n",
        "\n",
        " A method to model the relationship between a dependent variable and one or more independent variables using a straight line."
      ],
      "metadata": {
        "id": "ZF8-g_UE5hTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " ## MAE (Mean Absolute Error):\n",
        "\n",
        " The average of absolute differences between predicted and actual values, measuring model accuracy in the same units as the data.\n",
        "\n",
        "\n",
        "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|^2$$"
      ],
      "metadata": {
        "id": "-sI8vIQI5jzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### MAE Example:\n",
        " If predicted prices are $200k, $250k and actual prices are $210k, $240k, the MAE is $10k."
      ],
      "metadata": {
        "id": "GSJVGbnPRPvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSE (Root Mean Squared Error):\n",
        "The square root of the average squared differences between predicted and actual values, combining the advantages of MSE and interpretability in original units.\n",
        "\n",
        "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n"
      ],
      "metadata": {
        "id": "B_LXuxJzP-Es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RMSE Example:\n",
        " If the MSE is $100k², the RMSE is $10k."
      ],
      "metadata": {
        "id": "eqUg0XzMRxfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Difference Between Point and Plane:\n",
        "The vertical distance between a data point and the predicted value on a regression line or plane.\n",
        "\n"
      ],
      "metadata": {
        "id": "FHBtrtqeQGbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RnVPLlRC2ojd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instance-Based Learning:\n",
        " A learning approach where the model memorizes instances of the training data and makes predictions by comparing new instances to stored ones (e.g., k-NN)."
      ],
      "metadata": {
        "id": "4PVUKD1CQLWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model-Based Learning:\n",
        "A learning approach where the model learns a function or rule from the training data to make predictions on new data (e.g., linear regression)."
      ],
      "metadata": {
        "id": "nKBQuVHoQRdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Slope:\n",
        " The rate at which the dependent variable changes with respect to the independent variable, indicating the steepness of the line."
      ],
      "metadata": {
        "id": "gPrZONT_QYTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cost Function:\n",
        " A function that measures the error of the model's predictions, used to guide the optimization process (e.g., MSE in linear regression).\n",
        "\n",
        "\n",
        " $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$"
      ],
      "metadata": {
        "id": "CGtIxRzdQflx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cost Function Example:\n",
        " Using MSE to measure the error between predicted and actual house prices and adjusting the model to minimize this error.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4nBt_NtNR5y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convergence Algorithm:\n",
        " An algorithm that iteratively adjusts model parameters to minimize the cost function, like gradient descent."
      ],
      "metadata": {
        "id": "i6kOFZbDQlvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convergence Algorithm Example:\n",
        " Using gradient descent to adjust the line of best fit until the error cannot be reduced further."
      ],
      "metadata": {
        "id": "LKAyl901R_TC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performance Metrics:\n",
        "Measures used to evaluate the accuracy and effectiveness of a model (e.g., MAE, MSE, RMSE)."
      ],
      "metadata": {
        "id": "0E6ClhQuQ19A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performance Metrics Example:\n",
        "Evaluating a model predicting house prices by calculating its RMSE."
      ],
      "metadata": {
        "id": "2Szk7DueSFS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization\n",
        "\n",
        "its one type of linear regerssion used to prevent overfitting and imporve the  generalization performance of model\n",
        "\n",
        "\n",
        "`Regularization  = loss funtion + Penalty`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6wfWOtfuVX4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge Regularization\n",
        "\n",
        "l2  Regularization adds a penalty term with the loss funtion , which keeps the magnitude of the models weights(coefficients) as small as possible\n",
        "\n"
      ],
      "metadata": {
        "id": "hxAuSAuGdljh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2$$\n",
        "\n"
      ],
      "metadata": {
        "id": "4pWyyCgRXPHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lasso  Regularization\n",
        "\n",
        "Lasso (L1) Regularization is a technique used in machine learning to prevent overfitting by adding a penalty equal to the absolute value of the coefficients to the cost function. It tends to reduce some coefficients to exactly zero, effectively selecting a simpler model by keeping only the most important features."
      ],
      "metadata": {
        "id": "BRCPUO3PeZwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|$$\n"
      ],
      "metadata": {
        "id": "Y3f4VIjLfwZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression (L2 Regularization)\n",
        "\n",
        "The cost function for Ridge regression adds a penalty equal to the sum of the squared values of the coefficients:\n",
        "\n",
        "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2$$\n",
        "\n",
        "---\n",
        "\n",
        "### Lasso Regression (L1 Regularization)\n",
        "\n",
        "The cost function for Lasso regression adds a penalty equal to the sum of the absolute values of the coefficients:\n",
        "\n",
        "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|$$\n",
        "\n",
        "---\n",
        "\n",
        "### Elastic Net Regularization\n",
        "\n",
        "The cost function for Elastic Net regularization combines both L1 and L2 penalties:\n",
        "\n",
        "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x_i) - y_i)^2 + \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\lambda_2 \\sum_{j=1}^{n} \\theta_j^2$$\n"
      ],
      "metadata": {
        "id": "B0rZtTEygF8K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rH1oZ_KiW_v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias\n",
        "The bias is the difference between our actual and predicted values"
      ],
      "metadata": {
        "id": "TdY4zx2eTU8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## variance\n",
        "Variance is the sensitivity of the model to data fluctuation and noises"
      ],
      "metadata": {
        "id": "mfJCPG-uUXhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Overfitting:\n",
        " When a model learns too much from the training data, capturing noise and details that don't generalize well to new data. Example: A complex model that performs well on training data but poorly on test data.\n",
        "\n",
        "##Underfitting:\n",
        " When a model is too simple and fails to capture the underlying pattern of the data, leading to poor performance on both training and test data. Example: A linear model used for data with a non-linear relationship."
      ],
      "metadata": {
        "id": "nCpvQKUATojR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSc9PAsYbZbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-validation** is a technique used to evaluate the performance of a machine learning model by dividing the data into subsets, training the model on some of these subsets, and testing it on the remaining ones. This helps in assessing how well the model generalizes to unseen data.\n",
        "\n",
        "### Types of Cross-Validation:\n",
        "\n",
        "1. **Holdout Validation**:\n",
        "   - **Definition**: The dataset is split into two parts: one for training and one for testing.\n",
        "   - **Example**: Split a dataset of 1000 samples into 800 for training and 200 for testing.\n",
        "\n",
        "2. **K-Fold Cross-Validation**:\n",
        "   - **Definition**: The dataset is divided into \\(k\\) equal parts (folds). The model is trained on \\(k-1\\) folds and tested on the remaining fold. This process is repeated \\(k\\) times, with each fold being used as the test set once.\n",
        "   - **Example**: For a 5-Fold CV, a dataset of 1000 samples is split into 5 parts of 200 each. The model is trained on 4 parts and tested on 1 part, repeated 5 times.\n",
        "\n",
        "3. **Leave-One-Out Cross-Validation (LOOCV)**:\n",
        "   - **Definition**: Each data point in the dataset is used as a test set while the remaining points are used as the training set. This is repeated for each data point.\n",
        "   - **Example**: For a dataset with 1000 samples, the model is trained on 999 samples and tested on 1, repeated 1000 times.\n",
        "\n",
        "4. **Stratified K-Fold Cross-Validation**:\n",
        "   - **Definition**: Similar to K-Fold but ensures that each fold has the same proportion of classes as the original dataset, which is useful for imbalanced datasets.\n",
        "   - **Example**: For a 5-Fold CV on a dataset with 70% class A and 30% class B, each fold will have the same 70/30 class distribution.\n",
        "\n",
        "5. **Time Series Cross-Validation**:\n",
        "   - **Definition**: Used for time series data where the order of data points matters. The training set is built using a rolling or expanding window approach to predict the next data points.\n",
        "   - **Example**: In a time series of 1000 data points, train on points 1-800 to predict 801-820, then use 1-820 to predict 821-840, and so on.\n",
        "\n",
        "### Sample Example:\n",
        "\n",
        "**5-Fold Cross-Validation Example**:\n",
        "- **Step 1**: Split a dataset of 100 samples into 5 folds (20 samples each).\n",
        "- **Step 2**: Train the model on 4 folds (80 samples) and test on the 1 remaining fold (20 samples).\n",
        "- **Step 3**: Repeat this process 5 times, using each fold as the test set once.\n",
        "- **Step 4**: Average the performance metrics across the 5 iterations to evaluate the model.\n",
        "\n",
        "Cross-validation provides a more reliable estimate of model performance by reducing the bias that might result from a single train-test split."
      ],
      "metadata": {
        "id": "N6fUxFLzm88u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Linear Regression\n",
        "\n",
        "## Definition\n",
        "Multiple linear regression is a statistical technique that models the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "## Formula\n",
        "\n",
        "The general formula for multiple linear regression is:\n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
        "$$\n",
        "\n",
        "\n",
        "## Key Points\n",
        "- **Purpose**: To predict the dependent variable using multiple factors.\n",
        "- **Coefficients**: Indicate the influence of each independent variable on the dependent variable.\n",
        "- **Assumption**: Linear relationship between dependent and independent variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "6qtIJnAwo4hW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "## Definition\n",
        "Logistic regression is a statistical method used for binary classification problems. It models the probability that a given input belongs to a certain class.\n",
        "\n",
        "## Mathematical Intuition\n",
        "\n",
        "1. **Sigmoid Function**:\n",
        "   Logistic regression uses the sigmoid function to map predicted values to probabilities between 0 and 1.\n",
        "\n",
        "   $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "   Where:\n",
        "   - **\\( \\sigma(z) \\)**: The sigmoid function output (probability).\n",
        "   - **\\( e \\)**: Base of the natural logarithm.\n",
        "   - **\\( z \\)**: Linear combination of the input features and coefficients.\n",
        "\n",
        "2. **Linear Model**:\n",
        "   The input features are combined linearly:\n",
        "\n",
        "   $$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n$$\n",
        "\n",
        "\n",
        "3. **Probability Prediction**:\n",
        "   The probability of the positive class is given by:\n",
        "\n",
        "   $$P(y = 1 \\mid x) = \\sigma(z) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}}$$\n",
        "\n",
        "   The probability of the negative class is:\n",
        "\n",
        "   $$P(y = 0 \\mid x) = 1 - \\sigma(z)$$\n",
        "\n",
        "4. **Decision Boundary**:\n",
        "   The model classifies input as class 1 if the predicted probability is greater than 0.5 and as class 0 otherwise.\n",
        "\n",
        "   $$\\text{Class} = \\begin{cases}\n",
        "   1 & \\text{if } \\sigma(z) > 0.5 \\\\\n",
        "   0 & \\text{otherwise}\n",
        "   \\end{cases}$$\n",
        "\n",
        "## Key Points\n",
        "- **Purpose**: To estimate the probability of a binary outcome.\n",
        "- **Sigmoid Function**: Maps predictions to a probability between 0 and 1.\n",
        "- **Decision Rule**: Uses a threshold (typically 0.5) to classify outcomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "rDWnYKoHp_iZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression with One-vs-Rest (OVR)\n",
        "\n",
        "## Definition\n",
        "Logistic Regression with One-vs-Rest (OVR) is used for multi-class classification problems. It involves training multiple binary classifiers, each for a single class versus all other classes.\n",
        "\n",
        "## Method\n",
        "\n",
        "1. **Binary Classifier for Each Class**:\n",
        "   - Train one logistic regression model for each class, treating it as the positive class and all other classes as the negative class.\n",
        "   - If there are \\(K\\) classes, \\(K\\) separate binary classifiers are trained.\n",
        "\n",
        "2. **Prediction**:\n",
        "   - Each classifier outputs a probability for its class.\n",
        "   - The class with the highest probability among all classifiers is chosen as the predicted class.\n",
        "\n",
        "## Mathematical Formulation\n",
        "\n",
        "1. **Binary Logistic Model**:\n",
        "   For class \\(k\\), the probability is:\n",
        "\n",
        "   $$P(y = k \\mid x) = \\frac{1}{1 + e^{-(\\beta_{0k} + \\beta_{1k} x_1 + \\beta_{2k} x_2 + \\dots + \\beta_{nk} x_n)}}$$\n",
        "\n",
        "2. **Prediction Rule**:\n",
        "   - Compute the probability for each class \\(k\\).\n",
        "   - Assign the input to the class with the highest probability:\n",
        "\n",
        "   $$\\text{Class} = \\arg\\max_k P(y = k \\mid x)$$\n",
        "\n",
        "## Key Points\n",
        "- **Purpose**: To extend logistic regression to multi-class problems.\n",
        "- **OVR Approach**: Treats each class separately and compares probabilities.\n",
        "- **Output**: The class with the highest probability is chosen.\n",
        "\n"
      ],
      "metadata": {
        "id": "UGbWkxwXr6PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search\n",
        "\n",
        "## Definition\n",
        "Grid Search is a method used to find the best hyperparameters for a machine learning model by testing all possible combinations from a predefined set of hyperparameter values.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Define a Grid**:\n",
        "   - Create a list of possible values for each hyperparameter.\n",
        "   - Example: For `C` in Support Vector Machines, you might test values `[0.1, 1, 10]`.\n",
        "\n",
        "2. **Create Combinations**:\n",
        "   - Combine all possible values of the hyperparameters into different sets.\n",
        "   - Example: If testing `C` and `gamma`, you test combinations like `(0.1, 0.01)`, `(0.1, 0.1)`, `(1, 0.01)`, and so on.\n",
        "\n",
        "3. **Evaluate**:\n",
        "   - Train and test the model for each combination of hyperparameters.\n",
        "   - Measure the performance using metrics like accuracy or F1 score.\n",
        "\n",
        "4. **Select Best**:\n",
        "   - Choose the combination that gives the best performance.\n",
        "\n",
        "## Example\n",
        "\n",
        "Imagine you want to find the best hyperparameters for a model with two parameters: `learning_rate` and `n_estimators`.\n",
        "\n",
        "- **Define Grid**:\n",
        "  - `learning_rate`: `[0.01, 0.1, 1]`\n",
        "  - `n_estimators`: `[50, 100, 200]`\n",
        "\n",
        "- **Combinations**:\n",
        "  - `(0.01, 50)`, `(0.01, 100)`, `(0.01, 200)`\n",
        "  - `(0.1, 50)`, `(0.1, 100)`, `(0.1, 200)`\n",
        "  - `(1, 50)`, `(1, 100)`, `(1, 200)`\n",
        "\n",
        "- **Evaluate**:\n",
        "  - Train and test the model for each combination.\n",
        "\n",
        "- **Select Best**:\n",
        "  - Choose the combination with the highest accuracy or best performance metric.\n",
        "\n",
        "## Key Points\n",
        "- **Purpose**: To find the best hyperparameters for a model.\n",
        "- **Process**: Test all combinations from a predefined grid.\n",
        "- **Outcome**: The combination that performs best on validation data is selected.\n",
        "\n"
      ],
      "metadata": {
        "id": "pIxgRSDVtRyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Pickling\n",
        "\n",
        "## Definition\n",
        "Model pickling is the process of saving a trained machine learning model to a file so it can be loaded and used later without retraining.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Train Your Model**:\n",
        "   - First, you train a machine learning model on your data.\n",
        "\n",
        "2. **Save the Model**:\n",
        "   - Use a method called \"pickling\" to save the trained model to a file.\n",
        "   - This process converts the model into a format that can be stored and transferred.\n",
        "\n",
        "3. **Load the Model**:\n",
        "   - When needed, you can load the saved model from the file.\n",
        "   - This allows you to use the model for making predictions without retraining.\n",
        "\n",
        "## Example\n",
        "\n",
        "1. **Training and Saving**:\n",
        "   ```python\n",
        "   import pickle\n",
        "   from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "   # Train a model\n",
        "   model = LogisticRegression()\n",
        "   model.fit(X_train, y_train)\n",
        "\n",
        "   # Save the model\n",
        "   with open('model.pkl', 'wb') as file:\n",
        "       pickle.dump(model, file)\n"
      ],
      "metadata": {
        "id": "SD5lhfc4u1zu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qX6mujPamZwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nbhmpVVVsswV"
      }
    }
  ]
}